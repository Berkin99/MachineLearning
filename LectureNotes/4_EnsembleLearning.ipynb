{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0fc96d",
   "metadata": {},
   "source": [
    "# **4. Ensemble Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd3276c",
   "metadata": {},
   "source": [
    "## **4. Ensemble Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00c681",
   "metadata": {},
   "source": [
    "**Ensemble Learning** in machine learning refers to the technique of combining multiple models—often called **weak learners** or **base learners**—to produce a more robust and accurate predictive model. The core idea is that a group of weak learners, when properly combined, can outperform a single strong learner.\n",
    "\n",
    "- Individual models (like decision trees, SVMs, etc.) might **overfit**, **underfit**, or **fail** to capture complex patterns.\n",
    "- Combining multiple models helps:\n",
    "  - Reduce **variance** (e.g., by averaging predictions).\n",
    "  - Reduce **bias** (e.g., by learning different perspectives).\n",
    "  - Improve **generalization**.\n",
    "\n",
    "### Ensemble Learning Methods\n",
    "\n",
    "#### **4.1. Bagging (Bootstrap Aggregating)**\n",
    "\n",
    "#### **4.2. Boosting**\n",
    "\n",
    "#### **4.3. Stacking (Stacked Generalization)**\n",
    "\n",
    "### Benefits\n",
    "- Better performance than individual models.\n",
    "- More stable and robust to noise.\n",
    "- Can adapt to both simple and complex data patterns.\n",
    "\n",
    "### Trade-offs\n",
    "- Increased **computational cost**.\n",
    "- Harder to **interpret** compared to a single model.\n",
    "- Requires **careful tuning** (especially in boosting and stacking).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d48dafe",
   "metadata": {},
   "source": [
    "## **4.1. Bagging**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7c5b0",
   "metadata": {},
   "source": [
    "(Bootstrap Aggregating)\n",
    "\n",
    "**Concept:** Bagging aims to **reduce variance** in models that have a tendency to overfit the training data. It achieves this by training multiple base learners independently on different random subsets of the training data, and then aggregating their predictions.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1.  **Bootstrap Sampling:** Multiple subsets of the original training data are created by randomly sampling with replacement. This means that some data points may be included multiple times in a single subset, while others may be left out. Each subset is of the same size as the original dataset.\n",
    "2.  **Independent Training:** A base learner (the same type of algorithm) is trained on each of these bootstrap samples independently.\n",
    "3.  **Aggregation:** The predictions from all the trained base learners are combined to make the final prediction.\n",
    "    * For **classification**, this is typically done by majority voting (the class predicted by the most learners is chosen).\n",
    "    * For **regression**, this is typically done by averaging the predictions of all the learners.\n",
    "\n",
    "**Key Characteristics of Bagging:**\n",
    "\n",
    "* **Parallel Training:** Base learners are trained in parallel, independently of each other.\n",
    "* **Reduces Variance:** By averaging or voting across multiple models trained on different data, bagging smooths out the predictions and reduces the impact of high variance in individual models, leading to better generalization.\n",
    "* **Less Effective Against Bias:** Bagging is not primarily designed to reduce bias. If the base learners are inherently biased, the ensemble will also likely be biased.\n",
    "* **Examples:** Random Forest (an ensemble of decision trees trained using bagging), Bagged Decision Trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34271b5f",
   "metadata": {},
   "source": [
    "## **4.2. Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23945f79",
   "metadata": {},
   "source": [
    "**Concept:** Boosting aims to **reduce bias** and improve the accuracy of models by sequentially training weak learners, where each new learner tries to correct the errors made by the previous learners.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1.  **Sequential Training:** Base learners are trained sequentially. Each new learner is trained on a modified version of the original data, focusing on the instances that were misclassified by the previous learners.\n",
    "2.  **Weighted Data:** Initially, all data points are assigned equal weights. After each base learner is trained, the weights of the misclassified instances are increased, making the next learner pay more attention to these difficult examples.\n",
    "3.  **Weighted Aggregation:** The predictions from all the trained base learners are combined to make the final prediction. However, unlike bagging, the contribution of each learner is often weighted based on its performance during training (more accurate learners have a higher influence).\n",
    "\n",
    "**Key Characteristics of Boosting:**\n",
    "\n",
    "* **Sequential Training:** Base learners are trained one after another, with each learner influenced by the performance of the previous ones.\n",
    "* **Reduces Bias:** By focusing on the misclassified instances, boosting iteratively improves the model's ability to capture complex relationships in the data and reduce bias.\n",
    "* **Can Reduce Variance (to some extent):** While the primary goal is bias reduction, boosting can sometimes also lead to a reduction in variance.\n",
    "* **Risk of Overfitting:** Boosting can be more prone to overfitting if the training process continues for too long, especially if the data contains a lot of noise.\n",
    "* **Examples:** AdaBoost (Adaptive Boosting), Gradient Boosting Machines (GBM), XGBoost, LightGBM, CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8753b73",
   "metadata": {},
   "source": [
    "## **4.3. Bagging vs Bootstarpping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b53de",
   "metadata": {},
   "source": [
    "\n",
    "| Feature           | Bagging                                  | Boosting                                     |\n",
    "| ----------------- | ---------------------------------------- | -------------------------------------------- |\n",
    "| **Goal** | Reduce variance, improve stability       | Reduce bias, improve accuracy                |\n",
    "| **Base Learners** | Trained independently (in parallel)      | Trained sequentially                         |\n",
    "| **Data Sampling** | Bootstrap sampling (with replacement)    | Weighted sampling based on previous errors    |\n",
    "| **Learner Weight** | Typically equal                          | Weighted based on performance                |\n",
    "| **Focus** | Reducing overfitting                     | Reducing underfitting                        |\n",
    "| **Examples** | Random Forest, Bagged Decision Trees      | AdaBoost, Gradient Boosting, XGBoost, etc. |\n",
    "\n",
    "In essence:\n",
    "\n",
    "* **Bagging** asks a diverse set of independent models to vote or average their predictions.\n",
    "* **Boosting** asks a sequence of models to learn from the mistakes of the previous ones, with each model focusing on the difficult cases.\n",
    "\n",
    "The choice between bagging and boosting (or even combining them) depends on the specific characteristics of the data and the goals of the machine learning task. Both are powerful tools in the machine learning practitioner's toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6eac14",
   "metadata": {},
   "source": [
    "## **4.4. Mathematical Foundations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a9490",
   "metadata": {},
   "source": [
    "### 1. Mathematical Foundations of Bagging\n",
    "\n",
    "Consider a training dataset $Z = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$, where $x_i \\in \\mathcal{X}$ is the input feature vector and $y_i \\in \\mathcal{Y}$ is the target variable (for classification or regression).\n",
    "\n",
    "**1.1 Bootstrap Sampling:**\n",
    "\n",
    "* Bagging starts by creating $B$ bootstrap samples $Z_1, Z_2, ..., Z_B$, each of size $n$, drawn with replacement from the original dataset $Z$.\n",
    "* Let $I_{ib}$ be an indicator variable such that $I_{ib} = 1$ if the $i$-th data point from $Z$ is included in the $b$-th bootstrap sample $Z_b$, and $I_{ib} = 0$ otherwise.\n",
    "* For each bootstrap sample $Z_b$, we train a base learner $h_b(x; \\Theta_b)$, where $\\Theta_b$ represents the parameters of the $b$-th model learned from $Z_b$. Since each $Z_b$ is different, the learned parameters $\\Theta_b$ will also generally be different.\n",
    "\n",
    "**1.2 Aggregation:**\n",
    "\n",
    "* **Regression:** The final prediction for a new input $x$ is the average of the predictions from all $B$ base learners:\n",
    "    $$H(x) = \\frac{1}{B} \\sum_{b=1}^{B} h_b(x; \\Theta_b)$$\n",
    "* **Classification:** The final prediction is typically the majority vote. Let $h_b(x; \\Theta_b) \\in \\{c_1, c_2, ..., c_K\\}$ be the predicted class by the $b$-th learner. The final prediction $H(x)$ is the class $c_k$ that maximizes the number of votes:\n",
    "    $$H(x) = \\arg\\max_{c_k \\in \\{c_1, ..., c_K\\}} \\sum_{b=1}^{B} \\mathbb{I}(h_b(x; \\Theta_b) = c_k)$$\n",
    "    where $\\mathbb{I}(\\cdot)$ is the indicator function.\n",
    "\n",
    "**1.3 Variance Reduction:**\n",
    "\n",
    "The primary mathematical justification for bagging's effectiveness lies in its ability to reduce the variance of the prediction. Let's consider a simplified scenario for regression. Assume we have $B$ independent and identically distributed (i.i.d.) estimators $h_b(x)$ of some true function $f(x)$, each with variance $\\sigma^2$. The variance of their average is:\n",
    "\n",
    "$$Var\\left(\\frac{1}{B} \\sum_{b=1}^{B} h_b(x)\\right) = \\frac{1}{B^2} \\sum_{b=1}^{B} Var(h_b(x)) = \\frac{1}{B^2} \\cdot B \\sigma^2 = \\frac{\\sigma^2}{B}$$\n",
    "\n",
    "This shows that as the number of base learners $B$ increases, the variance of the ensemble prediction decreases. While the base learners in bagging are not perfectly independent (due to the overlap in bootstrap samples), this intuition holds. Bagging effectively averages out the individual errors of the base learners, particularly the high-variance components.\n",
    "\n",
    "### 2. Mathematical Foundations of Boosting\n",
    "\n",
    "Boosting, in its various forms, relies on the principle of sequentially learning weak learners and combining them with weights. Let's focus on AdaBoost as a representative example.\n",
    "\n",
    "**2.1 AdaBoost Algorithm (Mathematical Formulation):**\n",
    "\n",
    "Given a training dataset $Z = \\{(x_1, y_1), ..., (x_n, y_n)\\}$ where $y_i \\in \\{-1, +1\\}$ for binary classification.\n",
    "\n",
    "1.  **Initialization:** Assign equal weights to all training instances: $w_{i}^{(1)} = \\frac{1}{n}$ for $i = 1, 2, ..., n$.\n",
    "\n",
    "2.  **Iteration for $t = 1, 2, ..., T$ (where $T$ is the number of boosting rounds):**\n",
    "    a.  Train a weak learner $h_t(x)$ on the training data with the current weights $w^{(t)}$.\n",
    "    b.  Calculate the weighted error rate of $h_t(x)$:\n",
    "        $$\\epsilon_t = \\sum_{i=1}^{n} w_{i}^{(t)} \\mathbb{I}(h_t(x_i) \\neq y_i)$$\n",
    "    c.  Compute the weight $\\alpha_t$ assigned to the classifier $h_t(x)$:\n",
    "        $$\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$$\n",
    "        Note that $\\alpha_t > 0$ if $\\epsilon_t < 0.5$ (better than random guessing), $\\alpha_t < 0$ if $\\epsilon_t > 0.5$, and $\\alpha_t = 0$ if $\\epsilon_t = 0.5$.\n",
    "    d.  Update the weights of the training instances:\n",
    "        $$w_{i}^{(t+1)} = \\frac{w_{i}^{(t)} \\exp(-\\alpha_t y_i h_t(x_i))}{Z_t}$$\n",
    "        where $Z_t = \\sum_{i=1}^{n} w_{i}^{(t)} \\exp(-\\alpha_t y_i h_t(x_i))$ is a normalization factor to ensure that $\\sum_{i=1}^{n} w_{i}^{(t+1)} = 1$.\n",
    "        Observe that if $h_t(x_i) = y_i$ (correct classification), the weight $w_i$ is decreased. If $h_t(x_i) \\neq y_i$ (incorrect classification), the weight $w_i$ is increased.\n",
    "\n",
    "3.  **Final Prediction:** The final strong classifier $H(x)$ is a weighted combination of the weak classifiers:\n",
    "    $$H(x) = \\text{sign}\\left(\\sum_{t=1}^{T} \\alpha_t h_t(x)\\right)$$\n",
    "\n",
    "**2.2 Bias Reduction:**\n",
    "\n",
    "Boosting focuses on the instances that are difficult to classify. By increasing their weights in subsequent iterations, the algorithm forces the weak learners to pay more attention to these misclassified examples. This sequential adaptation aims to correct the biases of the individual weak learners, leading to a strong learner with lower bias.\n",
    "\n",
    "**2.3 Connection to Loss Functions:**\n",
    "\n",
    "The AdaBoost algorithm can be viewed as iteratively minimizing an exponential loss function:\n",
    "\n",
    "$$L(H) = \\sum_{i=1}^{n} \\exp(-y_i f(x_i))$$\n",
    "\n",
    "where $f(x) = \\sum_{t=1}^{T} \\alpha_t h_t(x)$. In each iteration, AdaBoost chooses the weak learner $h_t(x)$ and its weight $\\alpha_t$ to best reduce this loss.\n",
    "\n",
    "**2.4 Gradient Boosting:**\n",
    "\n",
    "Gradient Boosting generalizes the idea of boosting to arbitrary differentiable loss functions. Instead of re-weighting data points, each new learner in Gradient Boosting is trained to predict the *residual errors* (the negative gradient of the loss function with respect to the current ensemble prediction) made by the previous ensemble.\n",
    "\n",
    "Let $F_{m-1}(x)$ be the ensemble prediction at step $m-1$. The $m$-th learner $h_m(x)$ is trained to fit the negative gradient of the loss function $L(y, F(x))$ evaluated at $F_{m-1}(x)$:\n",
    "\n",
    "$$r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)}$$\n",
    "\n",
    "The new ensemble prediction is then updated as:\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\nu_m h_m(x)$$\n",
    "\n",
    "where $\\nu_m$ is a learning rate that controls the contribution of the $m$-th learner. Common loss functions include squared error for regression and logistic loss for classification.\n",
    "\n",
    "### Master's Level Final Exam Questions: Bagging and Boosting\n",
    "\n",
    "Here are some challenging final exam questions that require a deep understanding of the mathematical and conceptual aspects of Bagging and Boosting:\n",
    "\n",
    "**Question 1 (Bagging - Theoretical Analysis):**\n",
    "\n",
    "Consider a regression problem with a true underlying function $f(x)$ and noisy observations $y_i = f(x_i) + \\epsilon_i$, where $\\mathbb{E}[\\epsilon_i] = 0$ and $Var(\\epsilon_i) = \\sigma_\\epsilon^2$. Let $h(x; Z_b)$ be a base learner trained on a bootstrap sample $Z_b$. Assume that $\\mathbb{E}_{Z_b}[h(x; Z_b)] = \\bar{h}(x)$ and $Var_{Z_b}(h(x; Z_b)) = \\sigma_h^2(x)$.\n",
    "\n",
    "a)  Show that the expected prediction of a single bagged estimator $H(x) = \\frac{1}{B} \\sum_{b=1}^{B} h(x; Z_b)$ (as $B \\to \\infty$) is $\\bar{h}(x)$.\n",
    "\n",
    "b)  Derive an expression for the variance of the bagged estimator $H(x)$ (as $B \\to \\infty$) in terms of $\\sigma_h^2(x)$ and the correlation $\\rho(x) = Corr(h(x; Z_i), h(x; Z_j))$ for $i \\neq j$ due to the overlap in bootstrap samples.\n",
    "\n",
    "c)  Discuss how the correlation between the base learners affects the variance reduction achieved by bagging. Under what conditions would bagging provide the most significant variance reduction?\n",
    "\n",
    "**Question 2 (Boosting - Generalization and Overfitting):**\n",
    "\n",
    "Consider the AdaBoost algorithm.\n",
    "\n",
    "a)  Derive the update rule for the instance weights $w_{i}^{(t+1)}$ by minimizing the exponential loss function with respect to $\\alpha_t$.\n",
    "\n",
    "b)  Explain the concept of the \"margin\" in the context of boosting. How does the margin relate to the generalization performance of the AdaBoost classifier?\n",
    "\n",
    "c)  Discuss the conditions under which boosting algorithms, despite their focus on reducing bias, can be prone to overfitting. What are some strategies to mitigate overfitting in boosting?\n",
    "\n",
    "**Question 3 (Comparing Bagging and Boosting):**\n",
    "\n",
    "a)  Mathematically compare and contrast the bias-variance decomposition of the expected error for a bagged ensemble versus a boosted ensemble. Under what circumstances would you expect one to outperform the other in terms of prediction accuracy?\n",
    "\n",
    "b)  Consider a scenario where the base learners are highly unstable (small changes in the training data lead to significant changes in the learned model). Which ensemble technique, bagging or boosting, would likely be more effective in this case and why? Provide a mathematical argument.\n",
    "\n",
    "c)  Can bagging be used with sequential learning algorithms, and can boosting be used with independently trained base learners? Discuss the theoretical and practical implications of such combinations.\n",
    "\n",
    "**Question 4 (Advanced Boosting Techniques):**\n",
    "\n",
    "a)  Explain the core idea behind Gradient Boosting Machines (GBM). Derive the update rule for the ensemble prediction when using a general differentiable loss function.\n",
    "\n",
    "b)  Discuss the advantages of using second-order derivative information in boosting algorithms like XGBoost. How does the inclusion of the Hessian matrix contribute to improved performance?\n",
    "\n",
    "c)  Consider a multi-class classification problem. How can boosting algorithms like AdaBoost or Gradient Boosting be adapted to handle more than two classes? Discuss the mathematical formulations involved.\n",
    "\n",
    "These questions require not only recalling the definitions and algorithms but also demonstrating a deeper understanding of the underlying mathematical principles, their implications for model behavior, and the trade-offs involved in choosing and applying these ensemble techniques. Good luck to the aspiring masters!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
