{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a39e35",
   "metadata": {},
   "source": [
    "# **1. KNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff68e45",
   "metadata": {},
   "source": [
    "- **Simplicity and Intuition**: KNN is a straightforward algorithm that relies on the principle of similarity to classify or predict the output based on nearby data points.\n",
    "  \n",
    "- **No Training Phase**: KNN does not require an explicit training phase. It memorizes the entire training dataset and classifies data based on proximity at prediction time.\n",
    "\n",
    "- **Flexibility in Distance Metrics**: It can use various distance metrics such as Euclidean, Manhattan, or Minkowski, making it flexible for different types of data.\n",
    "\n",
    "- **Performance and Sensitivity to K**: The model's performance heavily depends on the value of *k* (the number of neighbors). As *k* increases:\n",
    "  - The model becomes **smoother** and **less sensitive** to noise.\n",
    "  - Larger *k* reduces variance but increases bias, making it less sensitive to fluctuations in the training data.\n",
    "\n",
    "- **Scalability Issues**: KNN can be computationally expensive, especially with large datasets. The need to calculate distances to all training points during prediction can make it slow.\n",
    "\n",
    "- **High Memory Usage**: Since KNN stores the entire training dataset, memory usage can become a problem when dealing with large datasets.\n",
    "\n",
    "- **Ability to Handle Non-linear Decision Boundaries**: KNN works well for datasets where decision boundaries are non-linear, as it adapts to the data’s shape.\n",
    "\n",
    "- **Robustness to Outliers (with proper k)**: A higher *k* helps smooth the decision boundary, making the model more robust to outliers in the dataset.\n",
    "\n",
    "- **Lazy Learning**: It is a lazy learner, meaning no learning occurs until a query is made. This can make it less efficient in scenarios where many queries are required.\n",
    "\n",
    "- **Feature Scaling Sensitivity**: KNN is sensitive to the scale of features. Features with larger ranges dominate the distance calculations, so **feature normalization** is often necessary.\n",
    "\n",
    "- **Feature Selection**: It doesn’t perform explicit feature selection, but it can still be effective if the relevant features are carefully selected.\n",
    "\n",
    "- **Interpretability**: KNN provides a transparent decision-making process since it can easily explain its decision based on the nearest neighbors.\n",
    "\n",
    "- **Computational Complexity**:  \n",
    "  - **Training Phase**: O(1) (no actual training, only storing data)\n",
    "  - **Prediction Phase**: O(n) where *n* is the number of data points in the training set.\n",
    "\n",
    "- **Sensitivity to Noise**: For low *k* (such as 1NN), KNN can be very sensitive to noisy data points or outliers, leading to overfitting.\n",
    "\n",
    "### **Comparison with 1NN**:\n",
    "- **1NN**: Extremely sensitive to noise and outliers.\n",
    "- **Higher k (e.g., 3NN, 5NN, etc.)**: As *k* increases, the model becomes more stable, reducing the impact of noise and overfitting, but at the cost of losing some flexibility."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
